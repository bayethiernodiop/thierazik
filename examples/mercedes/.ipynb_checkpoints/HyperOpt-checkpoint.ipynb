{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK, space_eval\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK, space_eval\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "import threading as th\n",
    "import keyboard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "import os        \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class HyperOptimizer(object):\n",
    "    def __init__(self, search_space,X,y,scoring,trial_file,trial_step=1, trial_initial_step=1,\n",
    "     debug=False, n_split=3, score_multiplier=1, preprocess_steps = None):\n",
    "        assert isinstance(search_space,dict), \"searc_space need to be a dict\"\n",
    "        assert \"models_spaces\" in search_space, \"model or models need to be associated with the key models_spaces\"\n",
    "        self.search_space = search_space\n",
    "        self.keep_going = True\n",
    "        self.scoring = scoring\n",
    "        self.trial_step = trial_step# how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "        self.trial_initial_step = trial_initial_step  # initial max_trials. put something small to not have to wait\n",
    "        self.preprocess_steps = preprocess_steps\n",
    "        self.score_multiplier = score_multiplier\n",
    "        self.debug = debug\n",
    "        #self.trial_folder = os.path.join(thierazik.config[\"PATH\"],\"hp_trials\")\n",
    "        self.trial_folder = \"/home/thierno/Downloads/hp_trials\"\n",
    "        self.trial_file = trial_file\n",
    "        self.trial_file_path = os.path.join(self.trial_folder,self.trial_file)\n",
    "        self.X = X\n",
    "        self.y=y\n",
    "        self.cv_inner = KFold(\n",
    "            n_splits=n_split, \n",
    "            shuffle=True, \n",
    "            #random_state=thierazik.config[\"SEED\"], \n",
    "            random_state=54, \n",
    "        )\n",
    "        self.best_params=None\n",
    "    def get_acc_status(self,model, X_, y):\n",
    "    \n",
    "        # Proceed to the cross-validation\n",
    "        # cv_result is a dict : test_score, train_score, fit_time, score_time, estimator\n",
    "        cv_results = cross_validate(\n",
    "            model,\n",
    "            X_,\n",
    "            y,\n",
    "            cv=self.cv_inner,\n",
    "            scoring=self.scoring,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'loss': self.score_multiplier * cv_results['test_score'].mean(),\n",
    "            'loss_std': cv_results['test_score'].std(),\n",
    "            'status': STATUS_OK,\n",
    "        }\n",
    "\n",
    "    def obj_fnc(self,params):   \n",
    "        \"\"\"\n",
    "        The function that return the value to be minimzed by FMIN wrt hyperparams space\n",
    "        \"\"\" \n",
    "        X_train_ = self.X\n",
    "        # proceed to preprocessing\n",
    "        if(self.preprocess_steps):\n",
    "            X_train_ = self.preprocess_steps(params, self.X[:])\n",
    "        \n",
    "        # get all parameters, except the model\n",
    "        parameters = params['models_spaces'].copy()\n",
    "        del parameters['model']\n",
    "        \n",
    "        # instantiation of the classifier model with parameters\n",
    "        model = params['models_spaces']['model'](**parameters)\n",
    "        \n",
    "        # return loss and status\n",
    "        return(self.get_acc_status(model, X_train_, self.y))\n",
    "\n",
    "    def run_trials(self):\n",
    "        os.makedirs(self.trial_folder, exist_ok=True)\n",
    "        \n",
    "        try:  # try to load an already saved trials object, and increase the max\n",
    "            # use data path for this project\n",
    "            hypopt_trials = pickle.load(open(self.trial_file_path, \"rb\"))\n",
    "            print(\"Found saved Trials! Loading...\")\n",
    "            max_evals = len(hypopt_trials.trials) + self.trial_step\n",
    "            print(\"Rerunning from {} trials.\".format(len(hypopt_trials.trials)))\n",
    "            \n",
    "        except:  # create a new trials object and start searching\n",
    "            print(\"Unable to load previous trials...\")\n",
    "            hypopt_trials = Trials()\n",
    "            max_evals = self.trial_initial_step\n",
    "\n",
    "        # Optimization accross the search space\n",
    "        self.best_params = fmin(\n",
    "            self.obj_fnc,\n",
    "            space=self.search_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=max_evals,\n",
    "            trials=hypopt_trials\n",
    "        )\n",
    "\n",
    "        # save the trials object\n",
    "        with open(self.trial_file_path, mode=\"wb\") as f:\n",
    "            pickle.dump(hypopt_trials, f)\n",
    "            \n",
    "        # get the best_params\n",
    "        self.best_params = space_eval(self.search_space, self.best_params)\n",
    "        \n",
    "        # print the main results\n",
    "        if(self.debug):\n",
    "            print(\n",
    "                \"\\n----------------------\",\n",
    "                \"\\nAlgo:\", best_params['models_spaces']['model'],\n",
    "                \"\\nLoss:\", hypopt_trials.best_trial['result']['loss'],\n",
    "                \"\\nPreprocessing:\", best_params['preprocessing_steps'],\n",
    "                \"\\nModel params:\", best_params['models_spaces'],\n",
    "            )\n",
    "    def save_best_params(self):\n",
    "        global o\n",
    "        o=self.best_params\n",
    "        with open(self.trial_file_path.split(\".\")[0]+\"_best_params.txt\", mode=\"w\") as f:\n",
    "            o = copy.deepcopy(self.best_params)\n",
    "            o[\"models_spaces\"][\"model\"] = str(o[\"models_spaces\"][\"model\"]).split(\"'\")[1]\n",
    "            f.write(json.dumps(o))\n",
    "        with open(self.trial_file_path.split(\".\")[0]+\"_model_best_params.pickle\", mode=\"wb\") as f:\n",
    "            o = copy.deepcopy(self.best_params)\n",
    "            o = o[\"models_spaces\"]\n",
    "            o.pop('model', None)\n",
    "            pickle.dump(o,f)\n",
    "    def key_capture_thread(self):\n",
    "        # Blocks until you press 'ESC'.\n",
    "        keyboard.wait('esc')\n",
    "        self.keep_going = False\n",
    "        print('\\nInterruptingâ€¦ Please wait until shut down and the saving of the current trial state.')\n",
    "    \n",
    "    def optimize(self):\n",
    "        self.keep_going = True\n",
    "        \"\"\"\n",
    "        Call this method to run the trials and press ESC to stop the optimization\n",
    "        \"\"\"\n",
    "        th.Thread(target=self.key_capture_thread, args=(), name='key_capture_thread', daemon=True).start()\n",
    "        while self.keep_going:\n",
    "            print(\"\\nExecuting... Press 'ESC' key to interrupt.\")\n",
    "            self.run_trials()\n",
    "            if(not self.keep_going):\n",
    "                self.save_best_params()\n",
    "            \n",
    "        print('\\nSuccessfully interrupted! The optimization can be restarted with the same state using the saved file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=1000, n_features=10,\n",
    "                                    n_informative=5, n_redundant=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "space = {}\n",
    "models_spaces = {}\n",
    "\n",
    "# Initializing the search space for preprocessing steps\n",
    "space['preprocessing_steps'] = hp.choice(\n",
    "    \"preprocessing\",\n",
    "    [\n",
    "    {'scale':       hp.choice('scale', [True, False])},\n",
    "    {'normalize':       hp.choice('normalize', [True, False])},\n",
    "    {'robust_scaler':       hp.choice('robust_scaler', [True, False])},\n",
    "    ]\n",
    ")\n",
    "\n",
    "{\n",
    "    'scale':       hp.choice('scale', [True, False]),\n",
    "    'normalize':       hp.choice('normalize', [True, False]),\n",
    "}\n",
    "\n",
    "models_spaces['rf'] = { \n",
    "    'model':        RandomForestClassifier,\n",
    "    'max_depth':    hp.choice('rf_max_depth', range(1,20)),\n",
    "    'max_features': hp.choice('rf_max_features', range(1,3)),\n",
    "    'n_estimators': hp.choice('rf_n_estimators', range(10,50)),\n",
    "    'criterion':    hp.choice('rf_criterion', [\"gini\", \"entropy\"]),\n",
    "}\n",
    "\n",
    "### LOGISTIC REGRESSION\n",
    "models_spaces['logit'] = { \n",
    "    'model':          LogisticRegression,\n",
    "    'warm_start' :    hp.choice('logit_warm_start', [True, False]),\n",
    "    'fit_intercept' : hp.choice('logit_fit_intercept', [True, False]),\n",
    "    'tol' :           hp.uniform('logit_tol', 0.00001, 0.0001),\n",
    "    'C' :             hp.uniform('logit_C', 0.05, 3),\n",
    "    'solver' :        hp.choice('logit_solver', ['newton-cg', 'lbfgs', 'liblinear']),\n",
    "    'max_iter' :      hp.choice('logit_max_iter', range(100,1000)),\n",
    "    'multi_class' :   'auto',\n",
    "    'class_weight' :  'balanced',\n",
    "}\n",
    "space['models_spaces'] = hp.choice(\n",
    "        'models_spaces',\n",
    "        [ models_spaces[key] for key in models_spaces ] # \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_steps(params, X_):\n",
    "    from sklearn.preprocessing import Normalizer,StandardScaler, RobustScaler\n",
    "    \n",
    "    # print(params)\n",
    "    \n",
    "    if 'normalize' in params['preprocessing_steps']:\n",
    "        if params['preprocessing_steps']['normalize'] == True:\n",
    "            X_ = Normalizer().fit_transform(X_)\n",
    "        \n",
    "    if 'scale' in params['preprocessing_steps']:\n",
    "        if params['preprocessing_steps']['scale'] == True:\n",
    "            X_ = StandardScaler().fit_transform(X_)\n",
    "    if 'robust_scaler' in params['preprocessing_steps']:\n",
    "        if params['preprocessing_steps']['robust_scaler'] == True:\n",
    "            X_ = RobustScaler().fit_transform(X_)\n",
    "\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "optimizer = HyperOptimizer(search_space=space,X=X,y=y,scoring=\"accuracy\",trial_file=\"xxxx.hyperopt\",trial_step=1, trial_initial_step=1,\n",
    "     debug=False, n_split=3, score_multiplier=-1, preprocess_steps = preprocess_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing... Press 'ESC' key to interrupt.\n",
      "Unable to load previous trials...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.98it/s, best loss: -0.9370358382334429]\n",
      "\n",
      "Executing... Press 'ESC' key to interrupt.\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 1 trials.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.10it/s, best loss: -0.9370358382334429]\n",
      "\n",
      "Executing... Press 'ESC' key to interrupt.\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 2 trials.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.91it/s, best loss: -0.9370358382334429]\n",
      "\n",
      "Executing... Press 'ESC' key to interrupt.\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 3 trials.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 21.97it/s, best loss: -0.9370358382334429]\n",
      "\n",
      "Executing... Press 'ESC' key to interrupt.\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 4 trials.\n",
      "\n",
      "Interruptingâ€¦ Please wait until shut down and the saving of the current trial state.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 24.17it/s, best loss: -0.9370358382334429]\n",
      "\n",
      "Successfully interrupted! The optimization can be restarted with the same state using the saved file\n"
     ]
    }
   ],
   "source": [
    "optimizer.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.4",
    "jupytext_version": "1.2.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
